{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Management and Database Design INFO 6210\n",
    "\n",
    "\n",
    "## How to create a database in MongoDB ?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we will learn how to create, populateand query a MongoDB database. \n",
    "We will cover the following details in this article:\n",
    "\n",
    "**1. Installing MongoDB on MacOS**\n",
    "\n",
    "**2. Running MongoDB on MacOS**\n",
    "\n",
    "**3. Creating Collections in MongoDB in python**\n",
    "\n",
    "**4. Populating data in MongoDB**\n",
    "\n",
    "**5. How to query MongoDB**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install MongoDB on MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install homebrew package manager \n",
    "If you dont have homebrew packmanager on you mac install as shown below. \n",
    "1. Open terminal app from Spotlight Search\n",
    "    1. Press CMD + SPACE to open spotlight search \n",
    "    2. Type terminal and open app\n",
    "2. Type : `ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"`\n",
    "\n",
    "#### Install MongoDB with homebrew\n",
    "1. Update homebrew package database with: \n",
    ">`brew update`\n",
    "2. Install MongoDB\n",
    ">`brew install mongodb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MongoDB on MacOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run MongoDB, we run `mongod` command on terminal. If necessary, specify the path of the data directory as explained below.\n",
    "\n",
    "##### Specify the path of data directory\n",
    "If you want to specify the path of data directory, run the following command on terminal \n",
    ">`mongod --dbpath <path to data directory>`\n",
    "\n",
    "#### Using MongoDB on terminal \n",
    "To access MongoDB on terminal run the following command\n",
    ">`mongo`\n",
    "\n",
    "Here we can perform all the operations in MongoDB on terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Introduction to Document based databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB stored data in the form of BSON documents. BSON is a binary representation of JSON documents. \n",
    "a typical JSON document looks as shown below. \n",
    "```\n",
    "{\n",
    "   field1: value1,\n",
    "   field2: value2,\n",
    "   field3: value3,\n",
    "   ...\n",
    "   fieldN: valueN\n",
    "}\n",
    "```\n",
    "A collection of Documents is known as **Collection** in MongoDB.\n",
    "\n",
    "We can compare a __Table__ in SQL as a __Collection__ in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MongoDB in Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing all the required packages for running MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to MongoDB and switching to required Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27017)  #Connection to MongoDB\n",
    "db=client.project   #Switching to Database with name 'project'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article we will gather data from Twitter and store it in MongoDB.\n",
    "We get a lot of data in twitter JSON file. But, we will only extract the data that we need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we are creating a database to store information of USERs and TWEETs. We will create two collections, \n",
    "1. User \n",
    "2. Tweet \n",
    "\n",
    "Now **User** collection will hold all the information about the user and the **Tweet** collection will store all the data about a specific tweet.\n",
    "\n",
    "So, the collections will be as follows,\n",
    "```\n",
    "Collection user:\n",
    "{\n",
    "     \"user_id\":\"Data String\",\n",
    "     \"user_name\":\"Data String\",\n",
    "     \"followers\": Data Int,\n",
    "     \"following\": Data Int,\n",
    "     \"tweet_count\": Data Int,\n",
    "     \"hashtags\":[ ]\n",
    "}\n",
    "```\n",
    "```\n",
    "Collection tweet:\n",
    "{\n",
    "     \"tweet_id\": \"Data String\",\n",
    "     \"user_id\": \"Data String\",\n",
    "     \"tweet_contents\": \"Data String\",\n",
    "     \"urls\": \"Data String\",\n",
    "     \"date\": \"Data String\",\n",
    "     \"time\": \"Data String\",\n",
    "     \"favourites\": Data Int,\n",
    "     \"retweets\": Data Int,\n",
    "     \"hashtags\":[ ]\n",
    "}\n",
    "```\n",
    "\n",
    "Here we can see that we are storing all the data related to user in **user** collection. Similarly all the data related to tweet is stored in **tweet** collection\n",
    "\n",
    "You can also see that we are storing **Hashtags** inside a list in the JSON file. This is because JSON provides us the functionality implement this. Thus we can Store and Query lists in JSON on MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can populate the data to MongoDB using the **pymongo** package in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To populate data in MongoDB, we create a JSON file with all the data and insert it in MongoDB using the method \n",
    "\n",
    "`db.<Collection>.insert_one(<JSON data>)`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus a sample JSON data for Tweet will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{ \"tweet_id\": \"990004485060624384\", \"user_id\": \"@AngelHealthTech\", \"tweet_contents\": \"RT @diioannid: What s hot in #Wearables and #WearableTech | https://t.co/susj0K9qUe via @MikeQuindazzi   #BigData #HealthTech #DataScience…\", \"urls\": \"https://pwc.to/2Hnu6s9\", \"date\": \"2018-04-27\", \"time\": \"23:07:17\", \"favourites\": 0, \"retweets\": 12, \"hashtags\": [\"wearables\", \"wearabletech\", \"bigdata\", \"healthtech\", \"datascience\"] }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a sample JSON data for User will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{ \"user_id\": \"@ExpoDX\" , \"user_name\": \"DXWorldEXPO ®\", \"followers\": 521, \"following\": 0, \"tweet_count\": 3709, \"hashtags\": [\"clodnative\", \"apm\", \"linx\", \"serverless\", \"devops\", \"bigdata\", \"clod\", \"iot\", \"iiot\"]  }\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case I have transferred data from SQL database to MongoDB. \n",
    "\n",
    "##### There were following tables in SQL database\n",
    "\n",
    "`User`  \n",
    "```\n",
    "(user_id varchar(25) not null primary key , user_name varchar(50), followers int, following int, tweet_count int)\n",
    "```\n",
    "Here **user_id** is the Primary Key\n",
    "\n",
    "`Tweet` \n",
    "```\n",
    "(tweet_id varchar(25) not null primary key, user_id varchar(25),tweet_content varchar(300), urls varchar(100))\n",
    "```\n",
    "Here **tweet_id** is the Primary Key\n",
    "\n",
    "`Tweet_details`  \n",
    "```\n",
    "(tweet_id varchar(25) not null primary key, date varchar(10),time varchar(8), favourate int, retweets int )\n",
    "```\n",
    "Here **tweet_id** is the Primary Key\n",
    "\n",
    "`Tags` \n",
    "```\n",
    "(tag_id integer not null primary key autoincrement, tag_details varchar(50) )\n",
    "```\n",
    "Here **tag_id** is the Primary Key\n",
    "\n",
    "`User_tags`\n",
    "```\n",
    "(tag_no integer not null primary key autoincrement, user_id varchar(25), tag_id varchar(25))\n",
    "```\n",
    "Here **tag_no** is the Primary Key\n",
    "\n",
    "`Tweet_tags` \n",
    "```\n",
    "(tag_no integer not null primary key autoincrement, tweet_id varchar(25), tag_id varchar(25))\n",
    "```\n",
    "Here **tag_no** is the Primary Key\n",
    "\n",
    "\n",
    "The formatting of data in SQL is done to satisfy the conditions of normalization. \n",
    "\n",
    "Thus we have different tables of `USER` , `Tweet` and  `Tweet_details`.\n",
    "\n",
    "We also have tables for `Tags` which Tag_Id and Tag_details, we are then creating relational tables `user_tags` and `tweet_tags` to store data for all users.\n",
    "\n",
    "We also have tables of *mispellings* and *synonyms*. But we use these while populating tags for users, to see if there is any mistake in spelling or if any Synonym is being used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating Data into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Transfering Data** from SQL to MongoDB, we wrote the following function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have written a functions which extracts data from SQL in our required format and converts it into **JSON** file compatible for MongoDB and this **JSON** is directly inserted into the required document. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Populate User Collection with data from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAndPopulateUsers():\n",
    "\n",
    "    mainQuery=\"Select * from user\"\n",
    "\n",
    "    for row in cursor.execute(mainQuery):\n",
    "\n",
    "        output= \"SELECT tag_id from user_tags where user_id='\"+ row[0]+\"' \"\n",
    "        list = []\n",
    "        for row2 in cursor2.execute(output):\n",
    "            getTag=\"select tag_details from tags where tag_id='\"+row2[0]+\"'\"\n",
    "            tagName=cursor3.execute(getTag)\n",
    "            list.append(tagName.fetchone()[0])\n",
    "        res= ('{ \"user_id\": \"%s\" , \"user_name\": \"%s\", \"followers\": %d, \"following\": %d, \"tweet_count\": %d, \"hashtags\": %s  }' %(row[0],row[1],row[2],row[3],row[4], str(list).replace(\"'\",\"\\\"\").replace(\"u\",\"\").replace(\"\\\\\",\"\")))\n",
    "        print (res)\n",
    "        db.user.insert_one(json.loads(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the **mainquery** variable is used to extract all the data from the user table.\n",
    "\n",
    "For every user, we extract tags related to that user and add them to a list.\n",
    "We will insert this list into hashtags column directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Populate Tweet Document with data from SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getandPopulateTweets():\n",
    "\n",
    "    mainQuery=\" Select * from tweet inner join tweet_details where tweet.tweet_id=tweet_details.tweet_id\"\n",
    "\n",
    "    for row in cursor.execute(mainQuery):\n",
    "        output = \"SELECT tag_id from tweet_tags where tweet_id='\" + row[0] + \"' \"\n",
    "        list = []\n",
    "        for row2 in cursor2.execute(output):\n",
    "            getTag = \"select tag_details from tags where tag_id='\" + row2[0] + \"'\"\n",
    "            tagName = cursor3.execute(getTag)\n",
    "            list.append(tagName.fetchone()[0])\n",
    "        res= ('{ \"tweet_id\": \"%s\", \"user_id\": \"%s\", \"tweet_contents\": \"%s\", \"urls\": \"%s\", \"date\": \"%s\", \"time\": \"%s\", \"favourites\": %d, \"retweets\": %d, \"hashtags\": %s }'  %(row[0], row[1], row[2].replace(\"\\n\",\" \").replace(\"\\\"\",\"\"), row[3], row[5], row[6], row[7], row[8], str(list).replace(\"'\",\"\\\"\").replace(\"u\\\"\",\"\\\"\").replace(\"\\\\\",\"\")   ))\n",
    "        print (res)\n",
    "        db.tweet.insert_one(json.loads(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to User Document, we will populate the tweet document by extracting the data from SQL with queries and converting them to **JSON** format\n",
    "\n",
    "In the above two functions, it can be observed that, we are replacing few characters from the data and substituting with some another character, or we are adding an escape character. This is done to make the data compitable with JSON, i.e some inverted commas or double inverted commas can cause abnormal termination of string.\n",
    "\n",
    "On running the above two functions, we can populate data in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user=db.user\n",
    "tweet=db.tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we gave the cursor for db.user to a variable user and db.tweet to variable tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MongoDB, we use the following method to find and display data :\n",
    "\n",
    "`db.<collection>.find(<query>, <projection>)`\n",
    "\n",
    "Here, the projection parameter determines which fields are returned in the matching documents. The projection parameter takes a document of the following form:\n",
    "\n",
    "```\n",
    "{ field1: <value>, field2: <value> ... }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying sample data from User Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'user_id': u'@ExpoDX', u'user_name': u'DXWorldEXPO \\xae', u'hashtags': [u'clodnative', u'apm', u'linx', u'serverless', u'devops', u'bigdata', u'clod', u'iot', u'iiot'], u'followers': 521, u'following': 0, u'_id': ObjectId('5ada4426ec661529fc12d319'), u'tweet_count': 3709}\n",
      "{u'user_id': u'@TechNewsRprt', u'user_name': u'TechNews Report', u'hashtags': [u'infographic', u'digitaltransformation', u'ai', u'marketing', u'mwc18', u'ericssondigital', u'bigdata'], u'followers': 4045, u'following': 3059, u'_id': ObjectId('5ada4426ec661529fc12d31a'), u'tweet_count': 5882}\n",
      "{u'user_id': u'@ScopeOnline', u'user_name': u'SCOPE', u'hashtags': [u'artificialintelligence', u'deeplearning', u'machinelearning', u'atomation'], u'followers': 2635, u'following': 2127, u'_id': ObjectId('5ada4426ec661529fc12d31b'), u'tweet_count': 11505}\n",
      "{u'user_id': u'@nschaetti', u'user_name': u'NS.ai (Nils Schaetti)', u'hashtags': [u'ai', u'machinelearning', u'ia', u'bigdata', u'datascience', u'robots', u'robotic', u'intelligenceartificielle', u'deeplearning', u'artificialintelligence'], u'followers': 8332, u'following': 7928, u'_id': ObjectId('5ada4426ec661529fc12d31c'), u'tweet_count': 91810}\n",
      "{u'user_id': u'@gabrielanthonyp', u'user_name': u'GabrielAnthony P.', u'hashtags': [u'ai', u'machinelearning', u'mwc18', u'ericssondigital', u'bigdata', u'artificialintelligence', u'data', u'intelligence'], u'followers': 2391, u'following': 3939, u'_id': ObjectId('5ada4426ec661529fc12d31d'), u'tweet_count': 79443}\n",
      "{u'user_id': u'@AffideaIreland', u'user_name': u'Affidea Ireland', u'hashtags': [u'ai', u'machinelearning'], u'followers': 1976, u'following': 2736, u'_id': ObjectId('5ada4426ec661529fc12d31e'), u'tweet_count': 3924}\n",
      "{u'user_id': u'@freetoopt', u'user_name': u'freetoopt', u'hashtags': [u'data', u'intelligence', u'machinelearning', u'ai', u'mwc18', u'ericssondigital', u'bigdata', u'artificialintelligence', u'neralnetworks', u'algorithms', u'deeplearning', u'iot', u'internetofthings', u'smartcity', u'technology', u'datascience', u'analytics'], u'followers': 66, u'following': 0, u'_id': ObjectId('5ada4426ec661529fc12d31f'), u'tweet_count': 4557}\n",
      "{u'user_id': u'@Chuck_Moeller', u'user_name': u'Chuck@SteamGear', u'hashtags': [u'machinelearning', u'ai'], u'followers': 2220, u'following': 2153, u'_id': ObjectId('5ada4426ec661529fc12d320'), u'tweet_count': 32446}\n",
      "{u'user_id': u'@Shuks_07', u'user_name': u'Saurabh Shukla', u'hashtags': [u'artificialintelligence', u'ai', u'machinelearning', u'sapleonardo', u'clod', u'sap', u'iot', u'blockchain'], u'followers': 3412, u'following': 3225, u'_id': ObjectId('5ada4426ec661529fc12d321'), u'tweet_count': 3688}\n",
      "{u'user_id': u'@RelevantTrack', u'user_name': u'Relevant Track', u'hashtags': [u'artificialintelligence', u'ai', u'neralnetworks', u'deeplearning', u'iot', u'digitaltransformation', u'blockchain', u'bitcoin', u'robotics'], u'followers': 10780, u'following': 1013, u'_id': ObjectId('5ada4426ec661529fc12d322'), u'tweet_count': 45024}\n"
     ]
    }
   ],
   "source": [
    "output= user.find({}).limit(10)\n",
    "for data in list(output):\n",
    "    print data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are limit the output to 10 items with the `.limit(<number>)` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying sample data from Tweet Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'user_id': u'@ExpoDX', u'tweet_contents': u'RT @ExpoDX: AI, Monitoring and Digital Transformation with @SaboTaylorDiab  @Loom_Systems #CloudNative #APM #Linux #Serverless #DevOps #Dat\\u2026', u'hashtags': [u'cloudnative', u'apm', u'linux', u'serverless', u'devops'], u'tweet_id': u'968036614202150912', u'retweets': 2, u'urls': u'NO_URL', u'time': u'08:14:48', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc5e')}\n",
      "{u'user_id': u'@ExpoDX', u'tweet_contents': u'AI, Monitoring and Digital Transformation with @SaboTaylorDiab  @Loom_Systems #CloudNative #APM #Linux #Serverless\\u2026 https://t.co/5CaXydsQEp', u'hashtags': [u'cloudnative', u'apm', u'linux', u'serverless'], u'tweet_id': u'968036597634564096', u'retweets': 2, u'urls': u'https://twitter.com/i/web/status/968036597634564096', u'time': u'08:14:44', u'date': u'2018-02-26', u'favourites': 1, u'_id': ObjectId('5ada44f2ec661529fc12dc5f')}\n",
      "{u'user_id': u'@TechNewsRprt', u'tweet_contents': u'RT @TamaraMcCleary: Tech Trends That Will Impact Your Business In The Next 4 Years // #Infographic #DigitalTransformation #AI #Marketing #T\\u2026', u'hashtags': [u'infographic', u'digitaltransformation', u'ai', u'marketing'], u'tweet_id': u'968036559638482944', u'retweets': 77, u'urls': u'NO_URL', u'time': u'08:14:35', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc60')}\n",
      "{u'user_id': u'@ScopeOnline', u'tweet_contents': u'RT @gh_comp_aut: #ArtificialIntelligence #DeepLearning and #MachineLearning in Manufacturing and #Automation - the topic at the  Forum K\\xfcns\\u2026', u'hashtags': [u'artificialintelligence', u'deeplearning', u'machinelearning', u'automation'], u'tweet_id': u'968036540193673216', u'retweets': 2, u'urls': u'NO_URL', u'time': u'08:14:31', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc61')}\n",
      "{u'user_id': u'@nschaetti', u'tweet_contents': u'#AI and #MachineLearning in focus at SAS Summit - Saudi Gazette #AI #IA #BigData #DataScience #computerintelligence https://t.co/i8BBGuv2U0', u'hashtags': [u'ai', u'machinelearning', u'ia', u'bigdata', u'datascience'], u'tweet_id': u'968036522153869312', u'retweets': 0, u'urls': u'http://saudigazette.com.sa/article/529149/BUSINESS/AI-and-machine-learning-in-focus-at-SAS-Summit', u'time': u'08:14:26', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc62')}\n",
      "{u'user_id': u'@gabrielanthonyp', u'tweet_contents': u'RT @Ronald_vanLoon: A Brief History of #AI by @Francesco_AI @Medium |  https://t.co/tpgMb20rex  #MachineLearning #ML #Artificialintelligenc\\u2026', u'hashtags': [u'ai', u'machinelearning'], u'tweet_id': u'968036431066271744', u'retweets': 23, u'urls': u'http://bit.ly/2oNyCGm', u'time': u'08:14:05', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc63')}\n",
      "{u'user_id': u'@AffideaIreland', u'tweet_contents': u'RT @ehealthmgmt: Key role of #AI &amp; #machinelearning in the field of #imaging  @AffideaGroup  @affideaCMO  #ML https://t.co/0Zbbi7anPW', u'hashtags': [u'ai', u'machinelearning'], u'tweet_id': u'968036419095670787', u'retweets': 2, u'urls': u'https://iii.hm/gon', u'time': u'08:14:02', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc64')}\n",
      "{u'user_id': u'@freetoopt', u'tweet_contents': u'RT @Ronald_vanLoon: #Data in, #Intelligence out: #MachineLearning pipelines demystified by @syegulalp @javaworldcom |  https://t.co/72oEdya\\u2026', u'hashtags': [u'data', u'intelligence', u'machinelearning'], u'tweet_id': u'968036417887596544', u'retweets': 4, u'urls': u'NO_URL', u'time': u'08:14:01', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc65')}\n",
      "{u'user_id': u'@Chuck_Moeller', u'tweet_contents': u'The latest The KDnuggets Coffee Break! https://t.co/7OVmVvLzEf #machinelearning #ai', u'hashtags': [u'machinelearning', u'ai'], u'tweet_id': u'968036376263438336', u'retweets': 0, u'urls': u'https://paper.li/Chuck_Moeller/1447794833?edition_id=f925e160-1acc-11e8-ad1c-0cc47a0d15fd', u'time': u'08:13:52', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc66')}\n",
      "{u'user_id': u'@Shuks_07', u'tweet_contents': u'By 2020, #ArtificialIntelligence Will Touch Everything ! #AI #MachineLearning #ML #Digital #Analytics #Intelligent\\u2026 https://t.co/mShgCzAZzz', u'hashtags': [u'artificialintelligence', u'ai', u'machinelearning'], u'tweet_id': u'968036316238700544', u'retweets': 0, u'urls': u'https://twitter.com/i/web/status/968036316238700544', u'time': u'08:13:37', u'date': u'2018-02-26', u'favourites': 0, u'_id': ObjectId('5ada44f2ec661529fc12dc67')}\n"
     ]
    }
   ],
   "source": [
    "output= tweet.find({}).limit(10)\n",
    "for data in list(output):\n",
    "    print data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
